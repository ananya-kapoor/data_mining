# -*- coding: utf-8 -*-
"""CCAcceptance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NKZSBcvVNPJs7i4Y0kq6ZdceJtZLUBvG

# Credit Card Acceptance: Predictive Analysis

## Data Cleaning and Preparation
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_auc_score, recall_score, precision_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,VotingClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

#Importing dataset
ca = pd.read_csv('/content/drive/MyDrive/IE7275_Group21_Project/ccacceptance.csv', lineterminator= '\n')
ca = ca.rename(columns={'Is_Lead': 'Acceptance'})

ca

ca.info()

ca = ca.iloc[:, 1:]

ca.describe(include='all')

ca.isnull().sum()

ca.Credit_Product = ca.Credit_Product.dropna()

ca.Credit_Product.value_counts()

"""## Data Exploration and Visualization"""

sns.countplot(data = ca, x='Acceptance',palette = 'bright')

cat_features = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']

plt.figure(figsize=(22,25))
sns.set(font_scale= 1.2)
sns.set_style('ticks')

for i, feature in enumerate(cat_features):
    plt.subplot(3, 2, i+1)
    sns.countplot(data=ca, x=feature, hue='Acceptance', palette='tab10')
    if feature == 'Region_Code':
        plt.xticks(rotation=90)
    
sns.despine()

plt.figure(figsize=(25, 7))

sns.countplot(data=ca, x='Age', hue='Acceptance', palette='Paired')

plt.show()

plt.figure(figsize=(25, 7))

sns.barplot(x= pd.qcut(ca['Age'], 10), y= 'Avg_Account_Balance' , data=ca, ci= None, hue= 'Acceptance', palette='hls')

plt.show()

plt.figure(figsize=(14, 10))
sns.distplot(ca.Avg_Account_Balance)
plt.show()

plt.figure(figsize=(14, 10))
plt.boxplot(ca.Avg_Account_Balance)
plt.show()

def remove_outliers(df,col,k=2):
    mean = df[col].mean()
    global ca
    sd = df[col].std()
    final_list = [x for x in df[col]if (x> mean - k * sd)]
    final_list = [x for x in final_list if (x < mean + k * sd)]
    ca = df.loc[df[col].isin(final_list)];
    print(ca.shape)

remove_outliers(ca,'Avg_Account_Balance',k=2.5)

plt.figure(figsize=(14, 10))
plt.boxplot(ca.Avg_Account_Balance)
plt.show()

plt.figure(figsize=(14, 10))
sns.distplot(ca.Avg_Account_Balance)
plt.show()

obj_cols = ca[ca.select_dtypes(include='object').columns]

obj_cols

from sklearn.preprocessing import LabelEncoder, StandardScaler
le = LabelEncoder()

for i in obj_cols:
    le.fit(ca[i])
    ca[i] = le.transform(ca[i])

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()

min_max_scaler.fit(ca[['Avg_Account_Balance']])

ca.Avg_Account_Balance = min_max_scaler.transform(ca[['Avg_Account_Balance']])

ca.Acceptance.sum()

ca

corr = ca.corr()
plt.figure(figsize=(20,10))
sns.heatmap(corr, annot = True)

X = ca.iloc [:,0:-1]
y = ca.iloc[:,-1]

train_X, test_X, train_y, test_y  = train_test_split(X, y,random_state=111, test_size= 0.2)

log = LogisticRegression()
rf = RandomForestClassifier()
Ada = AdaBoostClassifier()
xgb1 = XGBClassifier()

log.fit(train_X, train_y)
rf.fit(train_X, train_y)
Ada.fit(train_X, train_y)
xgb.fit(train_X, train_y)

model = [log, rf, Ada, xgb]
y_pred = []

for i in model:
    pred = i.predict(test_X)
    y_pred.append(pred)

roc_score = []
acc= []
f1Sco = []
precision = []
recall = []
for i in y_pred:
    roc = roc_auc_score(test_y, i)
    acc1 = accuracy_score(test_y, i)
    f1Sco1 = f1_score(test_y, i)
    precision1 = precision_score(test_y,i)
    recall1 = recall_score(test_y,i)
    roc_score.append(roc)
    acc.append(acc1)
    f1Sco.append(f1Sco1)
    precision.append(precision1)
    recall.append(recall1)

pm = pd.DataFrame({'Model name': ['Logistic', 'Rf','Ada', 'Xgb'], "Roc_score": roc_score , "Accuracy": acc, "F1 Score": f1Sco, "Precision": precision, "Recall": recall})
pm

pm.to_csv(r'/content/drive/MyDrive/IE7275_Group21_Project/modelperformanceb4os.csv',index = None, header= True)

cf_matrix_log = confusion_matrix(y_pred[0], test_y)
cf_matrix_RF = confusion_matrix(y_pred[1], test_y)
cf_matrix_Ada = confusion_matrix(y_pred[2], test_y)
cf_matrix_Xgb = confusion_matrix(y_pred[3], test_y)

group_names = ['‘TN’','FP','FN','TP']
group_counts_log = ["{0:0.0f}".format(value) for value in
                cf_matrix_log.flatten()]
group_percentages_log = ["{0:.2%}".format(value) for value in
                     cf_matrix_log.flatten()/np.sum(cf_matrix_log)]
labels_log = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_log, group_percentages_log)]
labels_log = np.asarray(labels_log).reshape(2,2)
sns.heatmap(cf_matrix_log, annot=labels_log, fmt='', cmap='Blues')

group_counts_RF = ["{0:0.0f}".format(value) for value in
                cf_matrix_RF.flatten()]
group_percentages_RF = ["{0:.2%}".format(value) for value in
                     cf_matrix_RF.flatten()/np.sum(cf_matrix_RF)]
labels_RF = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_RF, group_percentages_RF)]
labels_RF = np.asarray(labels_RF).reshape(2,2)
sns.heatmap(cf_matrix_RF, annot=labels_RF, fmt='', cmap='Blues')

group_counts_Ada = ["{0:0.0f}".format(value) for value in
                cf_matrix_Ada.flatten()]
group_percentages_Ada = ["{0:.2%}".format(value) for value in
                     cf_matrix_Ada.flatten()/np.sum(cf_matrix_Ada)]
labels_Ada = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_Ada, group_percentages_Ada)]
labels_Ada = np.asarray(labels_Ada).reshape(2,2)
sns.heatmap(cf_matrix_Ada, annot=labels_Ada, fmt='', cmap='Blues')

group_counts_Xgb = ["{0:0.0f}".format(value) for value in
                cf_matrix_Xgb.flatten()]
group_percentages_Xgb = ["{0:.2%}".format(value) for value in
                     cf_matrix_Xgb.flatten()/np.sum(cf_matrix_Xgb)]
labels_Xgb = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_Xgb, group_percentages_Xgb)]
labels_Xgb = np.asarray(labels_Xgb).reshape(2,2)
sns.heatmap(cf_matrix_Xgb, annot=labels_Xgb, fmt='', cmap='Blues')



"""# Oversampling using SMOTE"""

from imblearn.over_sampling import SMOTE

train_X

sm = SMOTE(random_state=42)
os_X, os_y = sm.fit_resample(train_X, train_y)

from collections import Counter
print (f"Original dataset shape{Counter(y)}")
print (f"Oversampled dataset shape{Counter(os_y)}")

log.fit(os_X, os_y)
rf.fit(os_X, os_y)
Ada.fit(os_X, os_y)
xgb1.fit(os_X, os_y)

os_model = [rf, xgb1, rf2, xgb3]
os_y_pred = []

for i in os_model:
    pred = i.predict(test_X)
    os_y_pred.append(pred)

os_roc_score = []
os_acc= []
os_f1Sco = []
os_precision = []
os_recall = []
for i in os_y_pred:
    os_roc = roc_auc_score(test_y, i)
    os_acc1 = accuracy_score(test_y, i)
    os_f1Sco1 = f1_score(test_y, i)
    os_precision1 = precision_score(test_y,i)
    os_recall1 = recall_score(test_y,i)
    os_roc_score.append(os_roc)
    os_acc.append(os_acc1)
    os_f1Sco.append(os_f1Sco1)
    os_precision.append(os_precision1)
    os_recall.append(os_recall1)

os_pm = pd.DataFrame({'Model': ['Logistic Regression', 'Random Forest','Adaptive Boost Classifier', 'Extreme Gradient Boost Classifier', 'Random Forest Classifier'], 
              "Accuracy": os_acc, "Precision": os_precision,"AUROC": os_roc_score, "F1 Score": os_f1Sco, "Recall": os_recall})
os_pm

pd.DataFrame({'Model': ['Random Forest', 'XGBoost','Tuned Random Forest', 'Tuned XGBoost'], 
              "Accuracy": os_acc, "Precision": os_precision,"AUROC": os_roc_score, "F1 Score": os_f1Sco, "Recall": os_recall})

os_pm.to_csv(r'/content/drive/MyDrive/IE7275_Group21_Project/os_modelperformance.csv',index = None, header= True)



os_cf_matrix_log = confusion_matrix(os_y_pred[0], os_y)
os_cf_matrix_RF = confusion_matrix(os_y_pred[1], os_y)
os_cf_matrix_Ada = confusion_matrix(os_y_pred[2], os_y)
os_cf_matrix_Xgb = confusion_matrix(os_y_pred[3], os_y)

os_cf_matrix_rf1 = confusion_matrix(os_y_pred[2], test_y)
os_cf_matrix_Xgb1 = confusion_matrix(os_y_pred[3], test_y)

group_names = ['‘TN’','FP','FN','TP']
group_counts_Xgb1 = ["{0:0.0f}".format(value) for value in
                os_cf_matrix_Xgb1.flatten()]
group_percentages_Xgb1 = ["{0:.2%}".format(value) for value in
                     os_cf_matrix_Xgb1.flatten()/np.sum(os_cf_matrix_Xgb1)]
labels_Xgb1 = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_Xgb1, group_percentages_Xgb1)]
labels_Xgb1 = np.asarray(labels_Xgb1).reshape(2,2)
sns.heatmap(os_cf_matrix_Xgb1, annot=labels_Xgb1, fmt='', cmap='Blues')

group_counts_RF1 = ["{0:0.0f}".format(value) for value in
                os_cf_matrix_rf1.flatten()]
group_percentages_RF1 = ["{0:.2%}".format(value) for value in
                     os_cf_matrix_rf1.flatten()/np.sum(os_cf_matrix_rf1)]
labels_RF1 = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_RF1, group_percentages_RF1)]
labels_RF1 = np.asarray(labels_RF1).reshape(2,2)
sns.heatmap(os_cf_matrix_rf1, annot=labels_RF1, fmt='', cmap='Blues')

group_counts_os_RF = ["{0:0.0f}".format(value) for value in
                os_cf_matrix_RF.flatten()]
group_percentages_os_RF = ["{0:.2%}".format(value) for value in
                     os_cf_matrix_RF.flatten()/np.sum(os_cf_matrix_RF)]
labels_os_RF = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts_os_RF, group_percentages_os_RF)]
labels_os_RF = np.asarray(labels_os_RF).reshape(2,2)
sns.heatmap(os_cf_matrix_RF, annot=labels_os_RF, fmt='', cmap='Blues')







params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
    
}

xgb2 = XGBClassifier()

from sklearn.model_selection import RandomizedSearchCV
random_search=RandomizedSearchCV(xgb1,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)

random_search.fit(os_X,os_y)

random_search.best_params_

xgb3 = XGBClassifier(learning_rate = 0.2, colsample_bytree=0.7, gamma=0.2, max_depth=10, min_child_weight=5)

xgb3.fit(os_X,os_y)

from sklearn.model_selection import cross_val_score
score=cross_val_score(xgb3,test_X,test_y,cv=10)

score

"""### RF Param Tuning"""

from pprint import pprint
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 300, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)

from sklearn.model_selection import RandomizedSearchCV
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf1 = RandomForestClassifier()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf1, param_distributions = random_grid, n_iter = 5, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(os_X, os_y)

rf_random.best_params_

rf2 = RandomForestClassifier(bootstrap= False, max_depth= 60,max_features= 'auto', min_samples_leaf= 2, min_samples_split= 2, n_estimators= 222)

rf2.fit(os_X,os_y)